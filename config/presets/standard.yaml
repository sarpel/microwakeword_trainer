# ─────────────────────────────────────────────────────────────────
# STANDARD PRESET - Balanced quality and speed
# Use for: Production training with good quality/speed balance
# ─────────────────────────────────────────────────────────────────

# ─────────────────────────────────────────────────────────────────
# HARDWARE PARAMETERS (IMMUTABLE)
# ─────────────────────────────────────────────────────────────────
hardware:
  sample_rate_hz: 16000
  mel_bins: 40
  window_size_ms: 30
  window_step_ms: 10
  clip_duration_ms: 1000

# ─────────────────────────────────────────────────────────────────
# PATHS
# ─────────────────────────────────────────────────────────────────
paths:
  positive_dir: "${DATASET_DIR:-./dataset}/positive"
  negative_dir: "${DATASET_DIR:-./dataset}/negative"
  hard_negative_dir: "${DATASET_DIR:-./dataset}/hard_negative"
  background_dir: "${DATASET_DIR:-./dataset}/background"
  rir_dir: "${DATASET_DIR:-./dataset}/rirs"
  processed_dir: "${DATA_DIR:-./data}/processed"
  checkpoint_dir: "${CHECKPOINT_DIR:-./checkpoints}"
  export_dir: "${MODEL_EXPORT_DIR:-./models/exported}"

# ─────────────────────────────────────────────────────────────────
# TRAINING PARAMETERS
# ─────────────────────────────────────────────────────────────────
training:
  # Standard step-based training
  training_steps: [20000, 10000]
  learning_rates: [0.001, 0.0001]
  batch_size: 128
  eval_step_interval: 500
  steps_per_epoch: 1000  # Approximate steps per epoch for mining calculations

  # Class weights - hard negatives get higher weight to reduce false accepts
  positive_class_weight: [1.0, 1.0]
  negative_class_weight: [20.0, 20.0]
  hard_negative_class_weight: [40.0, 40.0]

  # SpecAugment disabled (using full audio augmentation)
  time_mask_max_size: [0, 0]
  time_mask_count: [0, 0]
  freq_mask_max_size: [0, 0]
  freq_mask_count: [0, 0]

  # Checkpoint selection
  minimization_metric: "ambient_false_positives_per_hour"
  target_minimization: 0.2
  maximization_metric: "average_viable_recall"

  # Hours of negative/background audio for FAH calculation
  ambient_duration_hours: 10.0

# ─────────────────────────────────────────────────────────────────
# MIXEDNET ARCHITECTURE
# ─────────────────────────────────────────────────────────────────
model:
  architecture: "mixednet"

  # okay_nabu architecture (official ESPHome MWW)
  first_conv_filters: 32
  first_conv_kernel_size: 5
  stride: 3
  pointwise_filters: "64,64,64,64"
  mixconv_kernel_sizes: "[5],[7,11],[9,15],[23]"
  repeat_in_block: "1,1,1,1"
  residual_connection: "0,0,0,0"

  # Regularization
  dropout_rate: 0.2
  l2_regularization: 0.0001

# ─────────────────────────────────────────────────────────────────
# AUGMENTATION
# ─────────────────────────────────────────────────────────────────
augmentation:
  # Time-domain augmentations
  SevenBandParametricEQ: 0.1
  TanhDistortion: 0.1
  PitchShift: 0.1
  BandStopFilter: 0.1
  AddColorNoise: 0.1
  AddBackgroundNoise: 0.75
  Gain: 1.0
  RIR: 0.5

  # Noise mixing parameters
  background_min_snr_db: -5
  background_max_snr_db: 10
  min_jitter_s: 0.195
  max_jitter_s: 0.205

  # Augmentation magnitude ranges
  eq_min_gain_db: -6.0
  eq_max_gain_db: 6.0
  distortion_min: 0.1
  distortion_max: 0.5
  pitch_shift_min_semitones: -2.0
  pitch_shift_max_semitones: 2.0
  band_stop_min_center_freq: 100.0
  band_stop_max_center_freq: 5000.0
  band_stop_min_bandwidth_fraction: 0.5
  band_stop_max_bandwidth_fraction: 1.99
  gain_min_db: -3.0
  gain_max_db: 3.0
  color_noise_min_snr_db: -5.0
  color_noise_max_snr_db: 10.0
  # Background sources - use your own dataset paths
  impulse_paths: ["./dataset/rirs"]
  background_paths: ["./dataset/background"]
  augmentation_duration_s: 3.2

# ─────────────────────────────────────────────────────────────────
# PERFORMANCE CONFIGURATION
# ─────────────────────────────────────────────────────────────────
performance:
  # GPU if available, otherwise use CPU with good resources
  gpu_only: false
  mixed_precision: true

  # Standard CPU resources
  num_workers: 16
  num_threads_per_worker: 2
  prefetch_factor: 8
  pin_memory: true
  max_memory_gb: 60

  # Standard parallelism
  inter_op_parallelism: 16
  intra_op_parallelism: 16

  # Profiling enabled
  enable_profiling: true
  profile_every_n_steps: 100
  profile_output_dir: "./profiles"

  # TensorBoard enabled
  tensorboard_enabled: true
  tensorboard_log_dir: "./logs"

# ─────────────────────────────────────────────────────────────────
# SPEAKER CLUSTERING (Enabled for standard quality)
# ─────────────────────────────────────────────────────────────────
speaker_clustering:
  enabled: true
  method: "agglomerative"
  embedding_model: "speechbrain/spkrec-ecapa-voxceleb"
  similarity_threshold: 0.72
  leakage_audit_enabled: true

# ─────────────────────────────────────────────────────────────────
# HARD NEGATIVE MINING (Enabled for standard quality)
# ─────────────────────────────────────────────────────────────────
hard_negative_mining:
  enabled: true
  fp_threshold: 0.8
  max_samples: 5000
  mining_interval_epochs: 5

# ─────────────────────────────────────────────────────────────────
# EXPORT SETTINGS
# ─────────────────────────────────────────────────────────────────
export:
  wake_word: "Hey Katya"
  author: "Sarpel GURAY"
  website: "https://github.com/sarpel/microwakeword-training-platform"
  trained_languages: ["en"]

  quantize: true
  inference_input_type: "int8"
  inference_output_type: "uint8"

  probability_cutoff: 0.97
  sliding_window_size: 5
  tensor_arena_size: 30000
  minimum_esphome_version: "2024.7.0"

# ─────────────────────────────────────────────────────────────────
# PREPROCESSING (VAD, splitting, duration filtering)
# ─────────────────────────────────────────────────────────────────
preprocessing:
  # Duration filtering
  min_duration_ms: 300.0
  max_duration_ms: 2000.0
  discarded_dir: "./discarded"

  # VAD trim settings (vad_trim_audio.py)
  vad_aggressiveness: 2       # 0=lenient, 3=aggressive
  vad_pad_ms: 200             # ms of silence padding to keep around speech
  vad_frame_ms: 30            # VAD frame size (10, 20, or 30 ms)

  # Background audio splitting (split_long_audio.py)
  split_max_chunk_ms: 2000.0
  split_min_chunk_ms: 500.0
  split_target_chunk_ms: 2000.0

# ─────────────────────────────────────────────────────────────────
# QUALITY SCORING THRESHOLDS
# ─────────────────────────────────────────────────────────────────
quality:
  # Clipping detection
  clip_threshold: 0.001
  max_clip_ratio: 0.01

  # Score-based filtering
  discard_bottom_pct: 5.0     # discard lowest 5% of files by WQI
  min_wqi: 0.0
  discarded_quality_dir: "./discarded/quality"

  # WADA-SNR threshold
  min_snr_db: -10.0

  # Silero VAD (full scorer)
  vad_speech_threshold: 0.3

  # DNSMOS thresholds (full scorer)
  dnsmos_min_ovrl: 0.0
  dnsmos_min_sig: 0.0
  dnsmos_cache_dir: "~/.cache/dnsmos"
